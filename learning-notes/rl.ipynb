{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is dedicated to exploring the field of reinforcement learning. Reinforcement learning is a subfield of machine learning that focuses on training agents to make sequential decisions in an environment to maximize a reward signal.\n",
    "\n",
    "In this notebook, we will be using the OpenAI Gym library, which provides a collection of environments for developing and testing reinforcement learning algorithms. Specifically, we will be working with the CartPole environment, where the goal is to balance a pole on a cart by applying appropriate actions.\n",
    "\n",
    "Throughout the notebook, we will cover various concepts and techniques in reinforcement learning, including random search, Q-learning, and epsilon-greedy policy. We will visualize the training progress, analyze the rewards obtained, and generate an animated GIF of the agent's performance.\n",
    "\n",
    "Let's dive into the exciting world of reinforcement learning and explore the capabilities of the CartPole environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Cartpole environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "# The four states are: cart position, cart velocity, pole angle, pole angular velocity\n",
    "# The two actions are: push cart to the left, push cart to the right\n",
    "print(num_states, num_actions)\n",
    "print(env.observation_space.high, env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(w, state):\n",
    "    return int(w @ state >= 0)\n",
    "\n",
    "weights = np.random.randn(num_states)\n",
    "state, _ = env.reset()\n",
    "action = get_action(weights, state)\n",
    "\n",
    "print('weights:', weights)\n",
    "print('state:', state)\n",
    "# an action of 0 means push cart to the left, an action of 1 means push cart to the right\n",
    "print('action:', action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, w):\n",
    "    state, _ = env.reset()\n",
    "    rewards = 0\n",
    "    for _ in range(500):\n",
    "        state, reward, done, _, _ = env.step(get_action(w, state))\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    return rewards\n",
    "\n",
    "def random_search(env, num_episodes):\n",
    "    best_weights = None\n",
    "    maximum_reward = 0\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        weights = np.random.randn(num_states)\n",
    "        reward = run(env, weights)\n",
    "        rewards.append(reward)\n",
    "        if reward > maximum_reward:\n",
    "            maximum_reward = reward\n",
    "            best_weights = weights\n",
    "    return best_weights, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Rewards per Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my experience, the sufficient weights are already gained after 50 episodes\n",
    "best_weights, rewards = random_search(env, 500)\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "frames = []\n",
    "for _ in range(5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        state, _, done, _, _ = env.step(get_action(best_weights, state))\n",
    "    imageio.mimsave('images/cartpole_random_search.gif', frames, fps=30)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](images/cartpole_random_search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fully:\n",
    "    def __init__(self, neurons, input_length, activation=None):\n",
    "        self.relu = activation == 'relu'\n",
    "        self.sigmoid = activation == 'sigmoid'\n",
    "        self.weights = np.random.randn(neurons, input_length) / np.sqrt(input_length)\n",
    "        self.weight_update = np.zeros_like(self.weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input.copy()\n",
    "        self.y = np.dot(self.weights, self.input)\n",
    "        if self.relu:\n",
    "            self.y[self.y < 0] = 0\n",
    "        if self.sigmoid:\n",
    "            self.y = 1.0 / (1.0 + np.exp(-self.y))\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, grad, _):\n",
    "        grad = grad.copy()\n",
    "        if self.relu:\n",
    "            grad[self.y <=0] = 0\n",
    "        if self.sigmoid:\n",
    "            grad = grad * self.y * (1 - self.y)\n",
    "        self.weight_update += np.outer(grad, self.input)\n",
    "        return np.dot(self.weights.T, grad)\n",
    "\n",
    "    def update_weights(self, lr):\n",
    "        self.weights -= lr * self.weight_update.copy()\n",
    "        self.weight_update = np.zeros_like(self.weights)\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, topology, learning_rate):\n",
    "        self.lr = learning_rate\n",
    "        self.net = topology\n",
    "\n",
    "    def update_weights(self):\n",
    "        for layer in self.net: layer.update_weights(self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net: x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        for layer in self.net: x = layer.forward(x)\n",
    "        for layer in reversed(self.net): y = layer.backward(y, self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "    def act(self, state):\n",
    "        prob = self.net.forward(state)\n",
    "        action = 1 if np.random.rand() < prob else 0\n",
    "        return action, prob\n",
    "\n",
    "    # We discount the rewards and normalize them\n",
    "    # Discounting the rewards is important because we want to give more weight to the rewards that are closer to the present\n",
    "    def _discount_rewards(self, rewards, gamma=0.99):\n",
    "        discounted = np.zeros_like(rewards)\n",
    "        reward = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            reward = reward * gamma + rewards[t]\n",
    "            discounted[t] = reward\n",
    "        # By normalizing the rewards, we can make the training more stable\n",
    "        discounted -= np.mean(discounted)\n",
    "        discounted /= np.std(discounted)\n",
    "        return discounted\n",
    "\n",
    "    def train(self, episodes, gamma=0.99):\n",
    "        total_rewards = []\n",
    "\n",
    "        for _ in range(episodes):\n",
    "            error_gradients = []\n",
    "            episode_states = []\n",
    "            episode_rewards = []\n",
    "            done, truncated = False, False\n",
    "\n",
    "            state, _ = env.reset()\n",
    "            while not (done or truncated):\n",
    "                action, prob = self.act(state)\n",
    "                # We want to encourage the actions that were taken\n",
    "                error_gradients.append(-(action-prob))\n",
    "                episode_states.append(state)\n",
    "                state, reward, done, truncated, _ = env.step(action)\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "            # After every episode, we update the weights\n",
    "            total_rewards.append(sum(episode_rewards))\n",
    "            error_gradients = np.vstack(error_gradients) * self._discount_rewards(np.vstack(episode_rewards), gamma)\n",
    "            for state, gradient in zip(episode_states, error_gradients):\n",
    "                self.net.backward(state, gradient)\n",
    "            # We update the weights in a batch\n",
    "            # This helps to stabilize the training, since the weights are updated less frequently\n",
    "            # and a single example does not have a big impact on the weights\n",
    "            self.net.update_weights()\n",
    "\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topology = [\n",
    "    Fully(10, 4, activation='relu'),\n",
    "    Fully(10, 10),\n",
    "    Fully(1, 10, activation='sigmoid')]\n",
    "net = Network(topology=topology, learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(net)\n",
    "rewards = agent.train(1000)\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "frames = []\n",
    "for _ in range(5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action, _ = agent.act(state)\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "    imageio.mimsave('images/cartpole_policy_learning.gif', frames, fps=30)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](images/cartpole_policy_learning.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
